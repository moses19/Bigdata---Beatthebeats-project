{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics with Python - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text analytics, also known as Natural Language Processing (NLP), refer to the analytics toward text-based data.\n",
    "\n",
    "Text-based communication has become one of the most common forms of expression. We email, text message, tweet, and update our statuses on a daily basis. As a result, unstructured text data has become extremely common, and analyzing large quantities of text data is now a key way to understand what people are thinking.\n",
    "\n",
    "One of the biggest breakthroughs required for achieving any level of artificial intelligence is to have machines which can process text data. Thankfully, the amount of text data being generated in this universe has exploded exponentially in the last few years.\n",
    "\n",
    "It has become imperative for an organization to have a structure in place to mine actionable insights from the text being generated. From social media analytics to risk management and cybercrime protection, dealing with text data has never been more important.\n",
    "\n",
    "In this article we will discuss different feature extraction methods, starting with some basic techniques which will lead into advanced Natural Language Processing techniques. We will also learn about pre-processing of the text data in order to extract better features from clean data.\n",
    "\n",
    "By the end of this article, you will be able to perform text operations by yourself. Let’s get started!\n",
    "\n",
    "Original tutorial from [link](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/), revised and adapted by Dr. Tao.\n",
    "ver 1.0, May 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Basic feature extraction using text data\n",
    "    - Load data\n",
    "    - Number of words\n",
    "    - Unique Words and Their Counts\n",
    "    - Other Counts of Words\n",
    "2. Basic Text Pre-processing of text data\n",
    "    - Lower case\n",
    "    - Removing Punctuation\n",
    "    - Removal of Stop Words\n",
    "    - Common words removal\n",
    "    - Rare words removal\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - Part-of-Speach Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Text Feature Extraction\n",
    "\n",
    "We can use text data to extract a number of features even if we don’t have sufficient knowledge of Natural Language Processing. So let’s discuss some of them in this section.\n",
    "\n",
    "Before starting, let’s quickly read the training file from the dataset in order to perform different tasks on it. In the entire article, we will use the *twitter sentiment dataset* provided with NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class we are playing with Twitter data, NLTK has the data embedded - we can just load them from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded as a JSON file - which can be intuitively loaded as a dict. Let's look at what are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear the data is divided into three parts:\n",
    "    - positive;\n",
    "    - negative (these two are used for training in sentiment analysis);\n",
    "    - tweets.20150430... \n",
    "    \n",
    "In this particular exercise, we want to focus on the **positive** tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what is in our *pos_tweets* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "Provide your code in the block below two list the first **10** tweets in the *pos_tweets* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "pos_tweets[:9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Number of Words\n",
    "\n",
    "The first fact we want to know about text are word counts - let us start counting each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first tweet has 15 words.\n"
     ]
    }
   ],
   "source": [
    "# This will return the count for the first tweet\n",
    "print('The first tweet has %s words.' % str(len(pos_tweets[0].split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Provide the word counts for the first **10** tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "# use enumerate() here\n",
    "#for i , s in enumerate((pos_tweets[:9]))\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this is not very data science like approach, so we are going to use a powerful tool provided in Python: **Pandas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...\n",
       "1  @Lamb2ja Hey James! How odd :/ Please call our...\n",
       "2  @DespiteOfficial we had a listen last night :)...\n",
       "3                               @97sides CONGRATS :)\n",
       "4  yeaaaah yippppy!!!  my accnt verified rqst has..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets = pd.DataFrame(pos_tweets, columns=['text'])\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count words in every tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count\n",
       "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...          15\n",
       "1  @Lamb2ja Hey James! How odd :/ Please call our...          24\n",
       "2  @DespiteOfficial we had a listen last night :)...          20\n",
       "3                               @97sides CONGRATS :)           3\n",
       "4  yeaaaah yippppy!!!  my accnt verified rqst has...          22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets['word_count'] = pos_tweets['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the number of characters in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>15</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>24</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>20</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>22</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count  char_count\n",
       "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...          15         111\n",
       "1  @Lamb2ja Hey James! How odd :/ Please call our...          24         126\n",
       "2  @DespiteOfficial we had a listen last night :)...          20         107\n",
       "3                               @97sides CONGRATS :)           3          20\n",
       "4  yeaaaah yippppy!!!  my accnt verified rqst has...          22         106"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets['char_count'] = pos_tweets.text.str.len()\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also extract another feature which will calculate the average word length of each tweet. This can also potentially help us in improving our model.\n",
    "\n",
    "Here, we simply take the sum of the length of all the words and divide it by the total length of the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>15</td>\n",
       "      <td>111</td>\n",
       "      <td>6.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>24</td>\n",
       "      <td>126</td>\n",
       "      <td>4.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>20</td>\n",
       "      <td>107</td>\n",
       "      <td>4.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>22</td>\n",
       "      <td>106</td>\n",
       "      <td>4.047619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count  char_count  \\\n",
       "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...          15         111   \n",
       "1  @Lamb2ja Hey James! How odd :/ Please call our...          24         126   \n",
       "2  @DespiteOfficial we had a listen last night :)...          20         107   \n",
       "3                               @97sides CONGRATS :)           3          20   \n",
       "4  yeaaaah yippppy!!!  my accnt verified rqst has...          22         106   \n",
       "\n",
       "   avg_word_length  \n",
       "0         6.466667  \n",
       "1         4.291667  \n",
       "2         4.400000  \n",
       "3         6.000000  \n",
       "4         4.047619  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use this function to calcualte the average word length in each tweet\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "# Length of the sentence should be used for this.\n",
    "    #return (sum(len(sentence)))\n",
    "pos_tweets['avg_word_length'] = pos_tweets.text.apply(lambda x: avg_word(x))\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this dataframe just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets.to_csv('pos_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Unique Words and Their Counts\n",
    "\n",
    "You may have observed that the split() function is not well at identifying words - so we are going to use a built-in method with NLTK to identify words - namely tokenization. \n",
    "\n",
    "Refer to this [link](https://www.nltk.org/book/ch03.html) for more information.\n",
    "\n",
    "You should have noticed from the results below that we can capture the emojis from tweets using the TweetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer() # This function is particularly used for tweets\n",
    "from collections import Counter\n",
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the count of unique words in the overall set of texts - namely **Corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'#FollowFriday': 25,\n",
       "         '@France_Inte': 1,\n",
       "         '@PKuchly57': 1,\n",
       "         '@Milipol_Paris': 1,\n",
       "         'for': 749,\n",
       "         'being': 58,\n",
       "         'top': 29,\n",
       "         'engaged': 7,\n",
       "         'members': 11,\n",
       "         'in': 481,\n",
       "         'my': 484,\n",
       "         'community': 29,\n",
       "         'this': 263,\n",
       "         'week': 70,\n",
       "         ':)': 3691,\n",
       "         '@Lamb2ja': 1,\n",
       "         'Hey': 55,\n",
       "         'James': 5,\n",
       "         '!': 1844,\n",
       "         'How': 24,\n",
       "         'odd': 1,\n",
       "         ':/': 5,\n",
       "         'Please': 23,\n",
       "         'call': 24,\n",
       "         'our': 130,\n",
       "         'Contact': 2,\n",
       "         'Centre': 1,\n",
       "         'on': 313,\n",
       "         '02392441234': 1,\n",
       "         'and': 660,\n",
       "         'we': 146,\n",
       "         'will': 168,\n",
       "         'be': 249,\n",
       "         'able': 8,\n",
       "         'to': 1065,\n",
       "         'assist': 1,\n",
       "         'you': 1341,\n",
       "         'Many': 13,\n",
       "         'thanks': 182,\n",
       "         '@DespiteOfficial': 1,\n",
       "         'had': 38,\n",
       "         'a': 888,\n",
       "         'listen': 10,\n",
       "         'last': 33,\n",
       "         'night': 57,\n",
       "         'As': 5,\n",
       "         'You': 97,\n",
       "         'Bleed': 1,\n",
       "         'is': 418,\n",
       "         'an': 110,\n",
       "         'amazing': 44,\n",
       "         'track': 5,\n",
       "         '.': 1341,\n",
       "         'When': 14,\n",
       "         'are': 184,\n",
       "         'Scotland': 2,\n",
       "         '?': 581,\n",
       "         '@97sides': 1,\n",
       "         'CONGRATS': 2,\n",
       "         'yeaaaah': 1,\n",
       "         'yippppy': 1,\n",
       "         'accnt': 2,\n",
       "         'verified': 1,\n",
       "         'rqst': 1,\n",
       "         'has': 43,\n",
       "         'succeed': 1,\n",
       "         'got': 60,\n",
       "         'blue': 6,\n",
       "         'tick': 1,\n",
       "         'mark': 1,\n",
       "         'fb': 1,\n",
       "         'profile': 2,\n",
       "         '15': 4,\n",
       "         'days': 33,\n",
       "         '@BhaktisBanter': 17,\n",
       "         '@PallaviRuhail': 8,\n",
       "         'This': 38,\n",
       "         'one': 113,\n",
       "         'irresistible': 2,\n",
       "         '#FlipkartFashionFriday': 17,\n",
       "         'http://t.co/EbZ0L2VENM': 1,\n",
       "         'We': 77,\n",
       "         \"don't\": 61,\n",
       "         'like': 209,\n",
       "         'keep': 35,\n",
       "         'lovely': 52,\n",
       "         'customers': 2,\n",
       "         'waiting': 19,\n",
       "         'long': 31,\n",
       "         'hope': 84,\n",
       "         'enjoy': 31,\n",
       "         'Happy': 104,\n",
       "         'Friday': 92,\n",
       "         '-': 213,\n",
       "         'LWWF': 1,\n",
       "         'https://t.co/smyYriipxI': 1,\n",
       "         '@Impatientraider': 1,\n",
       "         'On': 11,\n",
       "         'second': 8,\n",
       "         'thought': 24,\n",
       "         ',': 964,\n",
       "         'there': 75,\n",
       "         '’': 21,\n",
       "         's': 22,\n",
       "         'just': 152,\n",
       "         'not': 123,\n",
       "         'enough': 18,\n",
       "         'time': 105,\n",
       "         'DD': 1,\n",
       "         'But': 32,\n",
       "         'new': 126,\n",
       "         'shorts': 1,\n",
       "         'entering': 1,\n",
       "         'system': 2,\n",
       "         'Sheep': 1,\n",
       "         'must': 15,\n",
       "         'buying': 1,\n",
       "         'Jgh': 3,\n",
       "         'but': 141,\n",
       "         'have': 342,\n",
       "         'go': 71,\n",
       "         'Bayan': 1,\n",
       "         ':D': 658,\n",
       "         'bye': 4,\n",
       "         'act': 4,\n",
       "         'of': 403,\n",
       "         'mischievousness': 1,\n",
       "         'am': 67,\n",
       "         'calling': 6,\n",
       "         'the': 999,\n",
       "         'ETL': 1,\n",
       "         'layer': 1,\n",
       "         'in-house': 1,\n",
       "         'warehousing': 1,\n",
       "         'app': 13,\n",
       "         'Katamari': 1,\n",
       "         'Well': 24,\n",
       "         '…': 40,\n",
       "         'as': 98,\n",
       "         'name': 13,\n",
       "         'implies': 1,\n",
       "         ':p': 138,\n",
       "         '@wncer1': 1,\n",
       "         '@Defense_gouv': 1,\n",
       "         'influencers': 12,\n",
       "         'Who': 14,\n",
       "         \"Wouldn't\": 3,\n",
       "         'Love': 80,\n",
       "         'These': 8,\n",
       "         'Big': 10,\n",
       "         '...': 290,\n",
       "         'Juicy': 3,\n",
       "         'Selfies': 3,\n",
       "         'http://t.co/QVzjgd1uFo': 1,\n",
       "         'http://t.co/oWBL11eQRY': 1,\n",
       "         '@Mish23615351': 1,\n",
       "         'follow': 284,\n",
       "         '@jnlazts': 62,\n",
       "         '&': 174,\n",
       "         'http://t.co/RCvcYYO0Iq': 62,\n",
       "         'u': 228,\n",
       "         'back': 154,\n",
       "         '@jjulieredburn': 1,\n",
       "         'Perfect': 3,\n",
       "         'so': 234,\n",
       "         'already': 28,\n",
       "         'know': 141,\n",
       "         \"what's\": 11,\n",
       "         'Great': 32,\n",
       "         'opportunity': 3,\n",
       "         'junior': 2,\n",
       "         'triathletes': 1,\n",
       "         'aged': 1,\n",
       "         '12': 5,\n",
       "         '13': 6,\n",
       "         'at': 167,\n",
       "         'Gatorade': 1,\n",
       "         'series': 4,\n",
       "         'Get': 28,\n",
       "         'your': 320,\n",
       "         'entries': 2,\n",
       "         'http://t.co/of3DyOzML0': 1,\n",
       "         'Laying': 1,\n",
       "         'out': 110,\n",
       "         'greetings': 2,\n",
       "         'card': 7,\n",
       "         'range': 1,\n",
       "         'print': 1,\n",
       "         'today': 104,\n",
       "         'love': 184,\n",
       "         'job': 16,\n",
       "         ':-)': 701,\n",
       "         \"Friend's\": 1,\n",
       "         'lunch': 3,\n",
       "         'yummmm': 1,\n",
       "         '#Nostalgia': 1,\n",
       "         '#TBS': 1,\n",
       "         '#KU': 1,\n",
       "         '@RookieSenpai': 1,\n",
       "         '@arcadester': 1,\n",
       "         'it': 460,\n",
       "         'id': 3,\n",
       "         'conflict': 1,\n",
       "         'help': 32,\n",
       "         \"here's\": 13,\n",
       "         'screenshot': 1,\n",
       "         'working': 29,\n",
       "         '@oohdawg_': 1,\n",
       "         'Hi': 141,\n",
       "         'liv': 2,\n",
       "         ')': 525,\n",
       "         'Hello': 39,\n",
       "         'I': 890,\n",
       "         'need': 51,\n",
       "         'something': 24,\n",
       "         'can': 139,\n",
       "         'fm': 2,\n",
       "         'me': 330,\n",
       "         'Twitter': 13,\n",
       "         '—': 27,\n",
       "         'sure': 50,\n",
       "         'thing': 35,\n",
       "         'dm': 16,\n",
       "         'x': 66,\n",
       "         'http://t.co/W6Dy130BV7': 1,\n",
       "         '@MBandScott_': 1,\n",
       "         '@Eric_FLE': 1,\n",
       "         '@pointsolutions3': 1,\n",
       "         'followers': 31,\n",
       "         '@rossbreadmore': 1,\n",
       "         \"I've\": 30,\n",
       "         'heard': 8,\n",
       "         'Four': 2,\n",
       "         'Seasons': 1,\n",
       "         'pretty': 18,\n",
       "         'dope': 1,\n",
       "         'Penthouse': 1,\n",
       "         'obvs': 1,\n",
       "         '#Gobigorgohome': 1,\n",
       "         'Have': 89,\n",
       "         'fun': 55,\n",
       "         \"y'all\": 4,\n",
       "         '@gculloty87': 4,\n",
       "         'Yeah': 14,\n",
       "         'suppose': 5,\n",
       "         'she': 26,\n",
       "         'was': 133,\n",
       "         'lol': 34,\n",
       "         'Chat': 1,\n",
       "         'bit': 18,\n",
       "         'off': 32,\n",
       "         'Youth': 19,\n",
       "         'Job': 23,\n",
       "         'Opportunities': 19,\n",
       "         '>': 68,\n",
       "         '@tolajobjobs': 19,\n",
       "         '@maphisa301': 1,\n",
       "         '💅': 1,\n",
       "         '🏽': 2,\n",
       "         '💋': 2,\n",
       "         \"haven't\": 14,\n",
       "         'seen': 10,\n",
       "         'years': 12,\n",
       "         '@Bosslogic': 1,\n",
       "         '@amellywood': 1,\n",
       "         '@CW_Arrow': 1,\n",
       "         '@ARROWwriters': 1,\n",
       "         'Thank': 131,\n",
       "         '@johngutierrez1': 1,\n",
       "         'rest': 10,\n",
       "         'goes': 7,\n",
       "         'by': 42,\n",
       "         'quickly': 3,\n",
       "         'bed': 14,\n",
       "         'music': 11,\n",
       "         'fix': 5,\n",
       "         'now': 113,\n",
       "         'dream': 12,\n",
       "         'Spiritual': 1,\n",
       "         'Ritual': 1,\n",
       "         'Festival': 5,\n",
       "         '(': 82,\n",
       "         'Népal': 1,\n",
       "         'Beginning': 2,\n",
       "         'Line-up': 2,\n",
       "         'It': 44,\n",
       "         'left': 12,\n",
       "         'line-up': 2,\n",
       "         'y': 7,\n",
       "         'See': 57,\n",
       "         'more': 100,\n",
       "         ':': 249,\n",
       "         'http://t.co/QMNz62OEuc': 1,\n",
       "         '@ke7zum': 1,\n",
       "         'Sarah': 4,\n",
       "         'Send': 2,\n",
       "         'us': 109,\n",
       "         'email': 21,\n",
       "         'bitsy@bitdefender.com': 1,\n",
       "         \"we'll\": 14,\n",
       "         'asap': 4,\n",
       "         '@izzkamilhalda': 1,\n",
       "         'lols': 1,\n",
       "         'MY': 11,\n",
       "         'kik': 3,\n",
       "         'hatessuce': 1,\n",
       "         '32429': 1,\n",
       "         '#kik': 12,\n",
       "         '#kikme': 1,\n",
       "         '#lgbt': 2,\n",
       "         '#tinder': 1,\n",
       "         '#nsfw': 1,\n",
       "         '#akua': 1,\n",
       "         '#cumshot': 1,\n",
       "         'http://t.co/TnHJD36yzf': 1,\n",
       "         '@KalinWhite': 3,\n",
       "         'come': 40,\n",
       "         'house': 4,\n",
       "         '#nsn_supplements': 1,\n",
       "         'Effective': 1,\n",
       "         'press': 1,\n",
       "         'release': 7,\n",
       "         'distribution': 1,\n",
       "         'with': 221,\n",
       "         'results': 2,\n",
       "         '[': 10,\n",
       "         'link': 14,\n",
       "         'removed': 2,\n",
       "         ']': 10,\n",
       "         '#PressRelease': 1,\n",
       "         '#NewsDistribution': 1,\n",
       "         'BAM': 44,\n",
       "         '@BarsAndMelody': 44,\n",
       "         'Can': 58,\n",
       "         'bestfriend': 49,\n",
       "         '@969Horan696': 44,\n",
       "         'She': 48,\n",
       "         'loves': 50,\n",
       "         'lot': 76,\n",
       "         'Warsaw': 44,\n",
       "         '<3': 135,\n",
       "         'x46': 1,\n",
       "         'everyone': 50,\n",
       "         'watch': 21,\n",
       "         'documentary': 1,\n",
       "         'Earthlings': 1,\n",
       "         'YouTube': 4,\n",
       "         '@jamiefigsxx': 1,\n",
       "         '@MichelBauza': 1,\n",
       "         '@InvataOnline': 1,\n",
       "         'supports': 6,\n",
       "         'buuuuuuuut': 1,\n",
       "         'oh': 24,\n",
       "         'well': 55,\n",
       "         '@leisuremarkltd': 1,\n",
       "         '@NoshandQuaff': 1,\n",
       "         '@aktarislam': 1,\n",
       "         '@keanebrands': 1,\n",
       "         '@HeritageSilver': 1,\n",
       "         'looking': 35,\n",
       "         'forward': 28,\n",
       "         'visiting': 3,\n",
       "         'next': 46,\n",
       "         '#letsgetmessy': 1,\n",
       "         'Jo': 1,\n",
       "         '@sehunshinedaily': 1,\n",
       "         'if': 130,\n",
       "         'makes': 17,\n",
       "         'feel': 28,\n",
       "         'better': 50,\n",
       "         'i': 203,\n",
       "         'never': 31,\n",
       "         'nor': 1,\n",
       "         'see': 109,\n",
       "         'anyone': 6,\n",
       "         'kpop': 1,\n",
       "         'flesh': 1,\n",
       "         '@Joyster2012': 1,\n",
       "         '@CathStaincliffe': 1,\n",
       "         'Good': 68,\n",
       "         'girl': 24,\n",
       "         'Best': 16,\n",
       "         'wishes': 4,\n",
       "         '@_Kimimi': 1,\n",
       "         'A': 46,\n",
       "         'great': 138,\n",
       "         'reason': 11,\n",
       "         'epic': 2,\n",
       "         'soundtrack': 1,\n",
       "         '@AquaDesignGroup': 2,\n",
       "         'shout': 7,\n",
       "         'added': 9,\n",
       "         'video': 29,\n",
       "         '@YouTube': 11,\n",
       "         'playlist': 5,\n",
       "         'http://t.co/yzpfsMxUq0': 1,\n",
       "         'im': 37,\n",
       "         'twitch': 4,\n",
       "         'going': 63,\n",
       "         'league': 5,\n",
       "         '1': 71,\n",
       "         '/': 67,\n",
       "         '4': 20,\n",
       "         'Would': 23,\n",
       "         'dear': 13,\n",
       "         '#Jordan': 1,\n",
       "         '@FIRDOZ': 1,\n",
       "         '@VisitJordan': 2,\n",
       "         '@dannyprol': 1,\n",
       "         '@ABNORMAL_ANA92': 2,\n",
       "         'okay': 32,\n",
       "         '@sssniperwolf': 1,\n",
       "         'how': 58,\n",
       "         'fake': 1,\n",
       "         'gameplays': 1,\n",
       "         ';)': 27,\n",
       "         'haha': 38,\n",
       "         'Im': 15,\n",
       "         'kidding': 6,\n",
       "         'do': 131,\n",
       "         'good': 162,\n",
       "         'stuff': 13,\n",
       "         '@dennislami': 1,\n",
       "         '@Dicle_Aygur': 1,\n",
       "         'yeah': 33,\n",
       "         'exactly': 3,\n",
       "         'Our': 10,\n",
       "         'product': 3,\n",
       "         'line': 3,\n",
       "         '#etsy': 1,\n",
       "         'shop': 6,\n",
       "         'Check': 16,\n",
       "         'http://t.co/h8exCTLQxg': 1,\n",
       "         '#boxroomcrafts': 1,\n",
       "         '@PeakYourMind': 1,\n",
       "         'vacation': 6,\n",
       "         '@groovinshawn': 1,\n",
       "         'they': 51,\n",
       "         'rechargeable': 1,\n",
       "         'normally': 2,\n",
       "         'comes': 5,\n",
       "         'charger': 2,\n",
       "         'when': 76,\n",
       "         'buy': 8,\n",
       "         '@France_Espana': 1,\n",
       "         '@reglisse_menthe': 1,\n",
       "         '@CCI_inter': 1,\n",
       "         \"she's\": 7,\n",
       "         'asleep': 9,\n",
       "         'no': 71,\n",
       "         'talk': 21,\n",
       "         'sooo': 3,\n",
       "         'someone': 30,\n",
       "         'text': 10,\n",
       "         '@brynybrath': 1,\n",
       "         '@smallcappy': 1,\n",
       "         'Yes': 24,\n",
       "         'bet': 5,\n",
       "         \"he'll\": 4,\n",
       "         'fit': 2,\n",
       "         'after': 20,\n",
       "         'hearing': 2,\n",
       "         'her': 38,\n",
       "         'speech': 1,\n",
       "         'Pity': 2,\n",
       "         'Green': 1,\n",
       "         'gardens': 2,\n",
       "         'midnight': 1,\n",
       "         'sun': 6,\n",
       "         'beautiful': 36,\n",
       "         'canals': 1,\n",
       "         'dasvidaniya': 1,\n",
       "         'till': 17,\n",
       "         'visit': 13,\n",
       "         '@KeithRParsons': 1,\n",
       "         'scouting': 1,\n",
       "         'SG': 1,\n",
       "         'future': 10,\n",
       "         'WLAN': 1,\n",
       "         'pros': 1,\n",
       "         'conference': 1,\n",
       "         'here': 95,\n",
       "         'Asia': 1,\n",
       "         '@Cecilie_Hell': 1,\n",
       "         '@420evilangel': 1,\n",
       "         '@wazimotometal': 1,\n",
       "         '@durooooooo': 1,\n",
       "         '@spigranty': 1,\n",
       "         'change': 10,\n",
       "         'lollipop': 1,\n",
       "         '🍭': 1,\n",
       "         'Nez': 1,\n",
       "         '#AGNEZMO': 1,\n",
       "         'https://t.co/cGViMHSINz': 1,\n",
       "         '@LittleMix': 2,\n",
       "         'Follow': 19,\n",
       "         'Dream': 2,\n",
       "         '@Elemaaan': 1,\n",
       "         'oley': 1,\n",
       "         '\"': 264,\n",
       "         '@CowokAddict': 1,\n",
       "         'Mama': 1,\n",
       "         'only': 49,\n",
       "         'why': 22,\n",
       "         'stand': 3,\n",
       "         'stronger': 1,\n",
       "         'up': 138,\n",
       "         '@MacatangayApril': 1,\n",
       "         '@tk_kjk_kndr': 1,\n",
       "         '@boukendreamer': 1,\n",
       "         'OH': 1,\n",
       "         'GOD': 1,\n",
       "         'MISTY': 1,\n",
       "         'BABY': 2,\n",
       "         'IS': 5,\n",
       "         'SO': 9,\n",
       "         'CUTE': 1,\n",
       "         'http://t.co/HnbinH35ES': 1,\n",
       "         '@CarcassDrop': 1,\n",
       "         'Woohoo': 3,\n",
       "         \"Can't\": 13,\n",
       "         'wait': 40,\n",
       "         'signed': 4,\n",
       "         'yet': 13,\n",
       "         'or': 56,\n",
       "         'still': 47,\n",
       "         'thinking': 10,\n",
       "         'about': 112,\n",
       "         'MKa': 5,\n",
       "         'liam': 5,\n",
       "         'access': 3,\n",
       "         '@SyuhxdxTengku': 1,\n",
       "         'most': 24,\n",
       "         'welcome': 55,\n",
       "         'Stats': 60,\n",
       "         'day': 202,\n",
       "         'arrived': 65,\n",
       "         'follower': 46,\n",
       "         'NO': 61,\n",
       "         'unfollowers': 60,\n",
       "         'via': 85,\n",
       "         'http://t.co/p6K6SiH58a': 1,\n",
       "         '@sluttywife2': 1,\n",
       "         \"shouldn't\": 3,\n",
       "         'surprised': 2,\n",
       "         'figure': 3,\n",
       "         '@murtishaw': 1,\n",
       "         '@aqui_fr': 1,\n",
       "         '@FRTechStartups': 1,\n",
       "         '#HappyBirthdayEmilyBett': 1,\n",
       "         '@emilybett': 1,\n",
       "         'Wishing': 11,\n",
       "         'all': 197,\n",
       "         'best': 46,\n",
       "         'sweet': 13,\n",
       "         'talented': 4,\n",
       "         'https://t.co/humtC1tr3I': 1,\n",
       "         '2': 55,\n",
       "         'plans': 7,\n",
       "         'down': 24,\n",
       "         'drain': 1,\n",
       "         '@x123456789tine': 2,\n",
       "         '@5SOS_FAHUpdates': 3,\n",
       "         'gotta': 2,\n",
       "         'timezones': 1,\n",
       "         'parents': 3,\n",
       "         'proud': 11,\n",
       "         'least': 15,\n",
       "         'maybe': 13,\n",
       "         'sometimes': 5,\n",
       "         'get': 139,\n",
       "         'happy': 79,\n",
       "         'because': 36,\n",
       "         'grades': 2,\n",
       "         'al': 1,\n",
       "         'http://t.co/bJjeGIOOGU': 1,\n",
       "         'Grande': 1,\n",
       "         'https://t.co/gsaaxNpR8u': 1,\n",
       "         'Manila_bro': 2,\n",
       "         'chosen': 1,\n",
       "         'http://t.co/lKXZWXN1fb': 1,\n",
       "         'Thanks': 209,\n",
       "         '@syazwanzainal': 1,\n",
       "         'let': 34,\n",
       "         \"you're\": 66,\n",
       "         'around': 17,\n",
       "         '..': 129,\n",
       "         'side': 14,\n",
       "         'world': 21,\n",
       "         '#eh': 1,\n",
       "         'too': 127,\n",
       "         'take': 28,\n",
       "         'care': 13,\n",
       "         '@michae1green': 1,\n",
       "         '@superninjaalan': 1,\n",
       "         '@Doug_Laney': 1,\n",
       "         'finally': 13,\n",
       "         'fucking': 11,\n",
       "         'weekend': 66,\n",
       "         'REAL': 2,\n",
       "         '@LoLEsportspedia': 1,\n",
       "         'x45': 1,\n",
       "         'YES': 3,\n",
       "         'Joined': 1,\n",
       "         '#HushedCallWithFraydoe': 1,\n",
       "         'gift': 4,\n",
       "         'from': 103,\n",
       "         '@_Fraydoe': 1,\n",
       "         'Gotta': 2,\n",
       "         '@HushedApp': 3,\n",
       "         'http://t.co/VaLVe8omdT': 1,\n",
       "         '@YasLarry': 1,\n",
       "         'yeahhh': 1,\n",
       "         'DM': 17,\n",
       "         'make': 57,\n",
       "         'joined': 5,\n",
       "         '#HushedPinWithSammy': 2,\n",
       "         'Event': 2,\n",
       "         'Might': 3,\n",
       "         'Text': 4,\n",
       "         '@SammyWilk': 2,\n",
       "         'Luv': 2,\n",
       "         'U': 17,\n",
       "         'http://t.co/kGfgawdHy9': 1,\n",
       "         'really': 71,\n",
       "         'appreciate': 11,\n",
       "         'share': 18,\n",
       "         'https://t.co/MhK3B3wOpQ': 1,\n",
       "         '@xhebenkewu_9920': 1,\n",
       "         '@TomParker': 4,\n",
       "         'wow': 18,\n",
       "         'That': 28,\n",
       "         'tom': 1,\n",
       "         '@darlingIXHAI22': 1,\n",
       "         '@SyreenAnne': 1,\n",
       "         'https://t.co/wx7yRrE7sV': 1,\n",
       "         'http://t.co/HVVPhSYakA': 1,\n",
       "         '3': 34,\n",
       "         '@AmericanOGrain': 1,\n",
       "         '@PecomeP': 1,\n",
       "         '@APaulicand': 1,\n",
       "         '@ZaynZaynmalik30': 1,\n",
       "         'Gym': 1,\n",
       "         'Monday': 8,\n",
       "         \"can't\": 30,\n",
       "         'Likes': 2,\n",
       "         '@HarNiLiZaLouis': 1,\n",
       "         'invite': 13,\n",
       "         'join': 9,\n",
       "         'Scope': 5,\n",
       "         'influencer': 5,\n",
       "         'http://t.co/rZgZtQ2fJT': 3,\n",
       "         'Those': 2,\n",
       "         'friends': 27,\n",
       "         'themselves': 3,\n",
       "         'nudes': 1,\n",
       "         '@JacobWhitesides': 1,\n",
       "         'sleep': 39,\n",
       "         'http://t.co/RB8pMNgMEo': 1,\n",
       "         'My': 60,\n",
       "         'birthday': 42,\n",
       "         '@metalgear_jp': 1,\n",
       "         '@Kojima_Hideo': 1,\n",
       "         'want': 69,\n",
       "         'T-shirts': 1,\n",
       "         'They': 11,\n",
       "         'cool': 30,\n",
       "         '@AxeRade': 1,\n",
       "         'haw': 1,\n",
       "         'phela': 1,\n",
       "         'Mom': 2,\n",
       "         'obviously': 2,\n",
       "         'him': 45,\n",
       "         '@zaynmalik': 4,\n",
       "         'prince': 1,\n",
       "         'charming': 1,\n",
       "         'stage': 2,\n",
       "         'https://t.co/OnVFhzt5fZ': 1,\n",
       "         'luck': 26,\n",
       "         'http://t.co/XzsOGaC4zK': 1,\n",
       "         '@straz_das': 1,\n",
       "         '@DCarsonCPA': 1,\n",
       "         '@GH813600': 1,\n",
       "         '@twentyonepilots': 1,\n",
       "         '@fujirock_jp': 1,\n",
       "         'tylers': 1,\n",
       "         'hipster': 1,\n",
       "         'glasses': 3,\n",
       "         '@MartyRafenstein': 1,\n",
       "         'Marty': 2,\n",
       "         'Glad': 16,\n",
       "         '@UKBusinessLunch': 1,\n",
       "         'joining': 4,\n",
       "         'again': 57,\n",
       "         '@LAfitnessUKhelp': 1,\n",
       "         'done': 43,\n",
       "         'Its': 17,\n",
       "         'afternoon': 8,\n",
       "         'lets': 12,\n",
       "         'read': 22,\n",
       "         'Al': 2,\n",
       "         'Kahfi': 1,\n",
       "         'before': 25,\n",
       "         'finish': 2,\n",
       "         'OhmyG': 1,\n",
       "         'yaya': 3,\n",
       "         'dub': 1,\n",
       "         '@mainedcm': 1,\n",
       "         'doing': 28,\n",
       "         'stalk': 1,\n",
       "         'ig': 1,\n",
       "         'GONDOOO': 1,\n",
       "         'MOO': 1,\n",
       "         'TOLOGOOO': 1,\n",
       "         'HAHA': 3,\n",
       "         '@rozbabes': 1,\n",
       "         \"Here's\": 12,\n",
       "         'become': 6,\n",
       "         'Details': 3,\n",
       "         'http://t.co/ipJ2yOiGet': 4,\n",
       "         '@NGourd': 1,\n",
       "         '@Locita': 1,\n",
       "         '@D_Robert_Kelly': 1,\n",
       "         '@kevinthewhippet': 1,\n",
       "         '@Cassie_Spaniel': 1,\n",
       "         '@Bracken_Nelson': 1,\n",
       "         '@BellisimoBella1': 1,\n",
       "         '@SpanielHarry': 1,\n",
       "         'zzz': 1,\n",
       "         'xx': 41,\n",
       "         'Physiotherapy': 1,\n",
       "         'hashtag': 5,\n",
       "         'Custom': 1,\n",
       "         '@Cjlopez21': 1,\n",
       "         '💪': 1,\n",
       "         'Monica': 1,\n",
       "         'miss': 9,\n",
       "         'sounds': 17,\n",
       "         'morning': 62,\n",
       "         \"That's\": 24,\n",
       "         'takes': 5,\n",
       "         'x43': 1,\n",
       "         '@MandaScapinello': 1,\n",
       "         'definitely': 16,\n",
       "         'try': 24,\n",
       "         'tonight': 18,\n",
       "         'then': 42,\n",
       "         'took': 6,\n",
       "         'advice': 6,\n",
       "         'Treviso': 1,\n",
       "         '@morallosanthony': 1,\n",
       "         '@NIKKIERIOZZI': 1,\n",
       "         '@imPastel': 17,\n",
       "         'concert': 22,\n",
       "         'Let': 34,\n",
       "         'city': 22,\n",
       "         'country': 20,\n",
       "         \"I'll\": 83,\n",
       "         'start': 43,\n",
       "         'Fine': 3,\n",
       "         'gorgeous': 8,\n",
       "         'friend': 33,\n",
       "         ';': 14,\n",
       "         'xo': 2,\n",
       "         'https://t.co/JmpkZP2DaI': 1,\n",
       "         'Oven': 1,\n",
       "         'roasted': 1,\n",
       "         'garlic': 1,\n",
       "         'olive': 1,\n",
       "         'oil': 2,\n",
       "         'dried': 2,\n",
       "         'tomatoes': 1,\n",
       "         'some': 87,\n",
       "         'basil': 1,\n",
       "         'century': 1,\n",
       "         'tuna': 1,\n",
       "         'https://t.co/EsCc9QhLob': 1,\n",
       "         '@HostMyOffice': 1,\n",
       "         '@NigelPWhittaker': 1,\n",
       "         '@lemezma': 1,\n",
       "         '@TWBC_Business': 2,\n",
       "         '@_TheBunkerJL': 1,\n",
       "         'Right': 3,\n",
       "         'atchya': 1,\n",
       "         '@TeamTall17': 1,\n",
       "         '@FlashHayer': 1,\n",
       "         \"doesn't\": 15,\n",
       "         'even': 32,\n",
       "         'The': 82,\n",
       "         'almost': 5,\n",
       "         'https://t.co/rolE3ZCL97': 1,\n",
       "         '@Michelploria': 1,\n",
       "         '@MyFrenchCity': 1,\n",
       "         '@jasoncreation': 1,\n",
       "         'No': 35,\n",
       "         'chance': 6,\n",
       "         '@ChiAB2486': 1,\n",
       "         '@MrCliveC': 1,\n",
       "         '@PCDKirkwood': 1,\n",
       "         '@DC_ARVSgt': 1,\n",
       "         '@COPS_President': 1,\n",
       "         '@EmWilliamsCCCU': 1,\n",
       "         '@LauraRGallagher': 1,\n",
       "         '@Hall11Kate': 1,\n",
       "         '@LaMinx541': 1,\n",
       "         '@JohnTarbet71': 1,\n",
       "         'cheers': 3,\n",
       "         '@thatguycalledP': 1,\n",
       "         'po': 4,\n",
       "         'ice': 6,\n",
       "         'cream': 4,\n",
       "         '@19strawberry66': 2,\n",
       "         'agree': 11,\n",
       "         '100': 7,\n",
       "         '%': 9,\n",
       "         '@SpazzyTsukihara': 5,\n",
       "         'hehehehe': 2,\n",
       "         'thats': 11,\n",
       "         'point': 7,\n",
       "         'stay': 15,\n",
       "         'home': 25,\n",
       "         '@Gculloty87': 1,\n",
       "         '@digitalplace2be': 1,\n",
       "         '@intlboost': 1,\n",
       "         '@_lafontpresse': 1,\n",
       "         '@stayfaboo': 1,\n",
       "         'soon': 45,\n",
       "         'promise': 5,\n",
       "         'Web': 1,\n",
       "         'Whatsapp': 3,\n",
       "         'volta': 1,\n",
       "         'funcionar': 1,\n",
       "         'com': 2,\n",
       "         'iPhone': 5,\n",
       "         'jailbroken': 1,\n",
       "         '@crustyolddeen': 2,\n",
       "         'plan': 10,\n",
       "         'watching': 16,\n",
       "         'later': 15,\n",
       "         '34': 3,\n",
       "         'mins': 6,\n",
       "         'Leia': 1,\n",
       "         'appears': 2,\n",
       "         'hologram': 1,\n",
       "         'R2D2': 1,\n",
       "         'w': 10,\n",
       "         'message': 6,\n",
       "         'Obi': 1,\n",
       "         'Wan': 2,\n",
       "         'he': 50,\n",
       "         'sits': 3,\n",
       "         'Luke': 5,\n",
       "         '@Mburu__': 1,\n",
       "         'Inter': 1,\n",
       "         'UCL': 1,\n",
       "         'Arsenal': 2,\n",
       "         'Small': 2,\n",
       "         'team': 26,\n",
       "         'Just': 43,\n",
       "         'passing': 1,\n",
       "         '🚂': 1,\n",
       "         '@': 18,\n",
       "         'Dewsbury': 2,\n",
       "         'Railway': 1,\n",
       "         'Station': 1,\n",
       "         'DEW': 1,\n",
       "         '@nationalrailenq': 1,\n",
       "         'West': 1,\n",
       "         'Yorkshire': 2,\n",
       "         'https://t.co/DvBssHbrfx': 1,\n",
       "         '@ClearlyArticle': 1,\n",
       "         'its': 48,\n",
       "         '430': 1,\n",
       "         'smh': 2,\n",
       "         '@uptommosass': 1,\n",
       "         \"it's\": 105,\n",
       "         '9:25': 1,\n",
       "         'live': 18,\n",
       "         'strange': 2,\n",
       "         'imagine': 3,\n",
       "         'what': 81,\n",
       "         'Megan': 1,\n",
       "         '@bookmyshow': 3,\n",
       "         '#MasaanToday': 6,\n",
       "         'A4': 3,\n",
       "         'Shweta': 1,\n",
       "         'Tripathi': 1,\n",
       "         '@WforWoman': 10,\n",
       "         '5': 17,\n",
       "         'Over': 2,\n",
       "         '20': 6,\n",
       "         'W': 8,\n",
       "         'kurtas': 1,\n",
       "         'And': 37,\n",
       "         'half': 6,\n",
       "         'number': 13,\n",
       "         '#WSaleLove': 14,\n",
       "         'Ah': 8,\n",
       "         'Larry': 3,\n",
       "         '@TransworldBooks': 1,\n",
       "         'https://t.co/8XhjJb4jtH': 1,\n",
       "         'Anyway': 1,\n",
       "         'kinda': 11,\n",
       "         'gooood': 1,\n",
       "         '@jhun_hunyo': 1,\n",
       "         'Lol': 17,\n",
       "         'life': 37,\n",
       "         'thank': 113,\n",
       "         'God': 14,\n",
       "         'enn': 1,\n",
       "         'https://t.co/GLoCEIjGQQ': 1,\n",
       "         '@OJBJ': 1,\n",
       "         '@holmesjsamuel': 1,\n",
       "         'surely': 3,\n",
       "         'could': 29,\n",
       "         'warmup': 1,\n",
       "         'Coming': 1,\n",
       "         '15th': 2,\n",
       "         'Bath': 3,\n",
       "         'Dum': 2,\n",
       "         'Andar': 1,\n",
       "         'Ram': 1,\n",
       "         'Sampath': 1,\n",
       "         'Sona': 1,\n",
       "         'Mohapatra': 1,\n",
       "         'Samantha': 1,\n",
       "         'Edwards': 1,\n",
       "         'Mein': 1,\n",
       "         'Tulane': 1,\n",
       "         'razi': 2,\n",
       "         'Wah': 1,\n",
       "         'Josh': 1,\n",
       "         'http://t.co/ul8MARDfhm': 1,\n",
       "         '@justinbieber': 9,\n",
       "         'always': 58,\n",
       "         'smile': 31,\n",
       "         '@darlakim_': 1,\n",
       "         'picture': 7,\n",
       "         '@arsenalnewsasit': 3,\n",
       "         '16.20': 1,\n",
       "         'right': 41,\n",
       "         'perfect': 19,\n",
       "         'timing': 5,\n",
       "         '@IzywayLesExpats': 1,\n",
       "         '@na4innov': 1,\n",
       "         '@InXpressCoAzur': 1,\n",
       "         '@Juleeyaanaa': 1,\n",
       "         '#GiveItUp': 1,\n",
       "         'given': 2,\n",
       "         'gas': 1,\n",
       "         'subsidy': 1,\n",
       "         'initiative': 1,\n",
       "         'proposed': 1,\n",
       "         'Feeling': 2,\n",
       "         'delighted': 3,\n",
       "         'having': 26,\n",
       "         'that': 246,\n",
       "         '@dayloladay': 1,\n",
       "         'missed': 4,\n",
       "         'yesterday': 7,\n",
       "         'x42': 1,\n",
       "         '@jaimeemelanie_': 1,\n",
       "         'lmaoo': 2,\n",
       "         'songs': 5,\n",
       "         'ever': 21,\n",
       "         'shall': 6,\n",
       "         'own': 6,\n",
       "         'little': 27,\n",
       "         'throwback': 2,\n",
       "         '@Clazziebritchas': 1,\n",
       "         'outlying': 1,\n",
       "         'islands': 1,\n",
       "         'such': 23,\n",
       "         'Cheung': 1,\n",
       "         'Chau': 1,\n",
       "         'Mui': 1,\n",
       "         'Wo': 1,\n",
       "         'totally': 9,\n",
       "         'different': 7,\n",
       "         '#KFCkitchentours': 2,\n",
       "         'Kitchen': 1,\n",
       "         'clean': 1,\n",
       "         \"I'm\": 161,\n",
       "         'amazed': 1,\n",
       "         '@KFC_India': 2,\n",
       "         '@cybelxxx': 1,\n",
       "         '@Jana7380': 1,\n",
       "         'cusp': 1,\n",
       "         'testing': 2,\n",
       "         'waters': 1,\n",
       "         'yours': 12,\n",
       "         'rewarding': 1,\n",
       "         '@BroadcastBeat': 1,\n",
       "         '@InSunWeTrust': 1,\n",
       "         '@conseilsmkg': 1,\n",
       "         'arummzz': 2,\n",
       "         \"Let's\": 14,\n",
       "         'drive': 5,\n",
       "         '#traveling': 3,\n",
       "         '#traveler': 3,\n",
       "         ...})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for text in pos_tweets.text:\n",
    "    n = tknzr.tokenize(text)\n",
    "    vocab.update(n)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique token (words) in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus has 15302 unique words.\n"
     ]
    }
   ],
   "source": [
    "print('The corpus has %s unique words.' % str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 unique words in the corpus, by their counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), ('!', 1844), ('you', 1341), ('.', 1341), ('to', 1065), ('the', 999), (',', 964), ('I', 890), ('a', 888), ('for', 749)]\n"
     ]
    }
   ],
   "source": [
    "print(vocab.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 1844,\n",
       " ',': 964,\n",
       " '.': 1341,\n",
       " ':)': 3691,\n",
       " 'I': 890,\n",
       " 'a': 888,\n",
       " 'for': 749,\n",
       " 'the': 999,\n",
       " 'to': 1065,\n",
       " 'you': 1341}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(vocab.most_common(10))\n",
    "\n",
    "# Most common are the puntuations , filler words articles\n",
    "# Rare words\n",
    "# Unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does above list make any sense to you? Maybe not.\n",
    "\n",
    "Nonetheless, we should visualize the counter of unique words just because we can.\n",
    "\n",
    "We should just visualze the most 20 words because of the long running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAD9CAYAAACSlFwNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1eedf2b3dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "count_dict = dict(vocab.most_common(10))\n",
    "words = range(len(count_dict))\n",
    "plt.bar(words, count_dict.values(), color='blue', align='center', tick_label=count_dict.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Other Counts of Words\n",
    "\n",
    "Sometimes we also care about other counts of words in a corpus, let's go through them.\n",
    "\n",
    "For instance, we noticed that the most common words from the counter (vocab) have less meanings - we call them **stopwords** (i.e. 'I', 'the', ...). Let's examine them by counts for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "                                                text  word_count  char_count  \\\n",
      "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...          15         111   \n",
      "\n",
      "   avg_word_length  stop_counts  \n",
      "0         6.466667            5  \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "# count of stop words in each sentence \n",
    "\n",
    "pos_tweets['stop_counts'] = pos_tweets.text.apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "pos_tweets.head()\n",
    "\n",
    "print(stop)\n",
    "print(pos_tweets[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for', 'being', 'in', 'my', 'this']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in pos_tweets.text[0].split() if x in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also noticed that a great amount of special characters, such as *emojis*, appeared in the tweets. We can count them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "t = pos_tweets.text[0]\n",
    "print(tknzr.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def count_special_char(sentence):\n",
    "    non_special = [tok for tok in tknzr.tokenize(sentence) if(re.search(r'^\\w' , tok))] # or (r'^\\@',tok))]\n",
    "    return(len(non_special))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_special_char(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>stop_counts</th>\n",
       "      <th>special_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>15</td>\n",
       "      <td>111</td>\n",
       "      <td>6.466667</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>24</td>\n",
       "      <td>126</td>\n",
       "      <td>4.291667</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>20</td>\n",
       "      <td>107</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>22</td>\n",
       "      <td>106</td>\n",
       "      <td>4.047619</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  word_count  char_count  \\\n",
       "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...          15         111   \n",
       "1  @Lamb2ja Hey James! How odd :/ Please call our...          24         126   \n",
       "2  @DespiteOfficial we had a listen last night :)...          20         107   \n",
       "3                               @97sides CONGRATS :)           3          20   \n",
       "4  yeaaaah yippppy!!!  my accnt verified rqst has...          22         106   \n",
       "\n",
       "   avg_word_length  stop_counts  special_char  \n",
       "0         6.466667            5             5  \n",
       "1         4.291667            8             5  \n",
       "2         4.400000            8             5  \n",
       "3         6.000000            0             2  \n",
       "4         4.047619            6             4  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets['special_char'] = pos_tweets.text.apply(lambda x: len(tknzr.tokenize(x)) - count_special_char(x))\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also care about the number of numerics in each tweet.\n",
    "\n",
    "Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  num_counts\n",
       "0  #FollowFriday @France_Inte @PKuchly57 @Milipol...           0\n",
       "1  @Lamb2ja Hey James! How odd :/ Please call our...           1\n",
       "2  @DespiteOfficial we had a listen last night :)...           0\n",
       "3                               @97sides CONGRATS :)           0\n",
       "4  yeaaaah yippppy!!!  my accnt verified rqst has...           1"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets['num_counts'] = pos_tweets.text.apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "pos_tweets[['text','num_counts']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Say we are interested in searching for all long words in the tweets. A long word is defined as:\n",
    "- has more than 5 letters;\n",
    "- has to be a word (not special character, numerics, ...)\n",
    "\n",
    "Please write you own code for the **count of long words** and add it as a column in *pos_tweets*.\n",
    "\n",
    "**HINT**: use code/results from above as much as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# define your function here\n",
    "#pos_tweets['num_counts'] = pos_tweets.text.apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "#pos_tweets['special_char'] = pos_tweets.text.apply(lambda x: len(tknzr.tokenize(x)) - count_special_char(x))\n",
    "\n",
    "\n",
    "#count_longwords = {}\n",
    "#for word in words:\n",
    " #   if len(word > 5\n",
    "  #  count_longwords += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def long_words(sentence):\n",
    "    non_special = [tok for tok in tknzr.tokenize(sentence) if(re.search(r'^\\w',tok)) ]\n",
    "    non_numeric = [tok for tok in non_special if not tok.isdigit()]\n",
    "    lng_words = [w for w in non_numeric if len(w)>5 ]\n",
    "    return(lng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['engaged', 'members', 'community']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use apply(lambda) here\n",
    "long_words(t)\n",
    "\n",
    "\n",
    "#pos_tweets['Long Words'] = pos_tweets.text.apply(lambda x: len(tknzr.tokenize(x)) - count_special_char(x))\n",
    "# to add the same in the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic Preprocessing\n",
    "\n",
    "So far, we have learned how to extract basic features from text data. Before diving into text and feature extraction, our first step should be cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our training data.\n",
    "\n",
    "So, let’s get into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Lower case\n",
    "\n",
    "The first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #followfriday @france_inte @pkuchly57 @milipol...\n",
       "1    @lamb2ja hey james! how odd :/ please call our...\n",
       "2    @despiteofficial we had a listen last night :)...\n",
       "3                                 @97sides congrats :)\n",
       "4    yeaaaah yippppy!!! my accnt verified rqst has ...\n",
       "Name: lower, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# No order prefernce with lower or upper \n",
    "\n",
    "pos_tweets['lower'] = pos_tweets.text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "pos_tweets.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Removing Punctuation\n",
    "\n",
    "The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    followfriday france_inte pkuchly57 milipol_par...\n",
       "1    lamb2ja hey james how odd please call our cont...\n",
       "2    despiteofficial we had a listen last night as ...\n",
       "3                                     97sides congrats\n",
       "4    yeaaaah yippppy my accnt verified rqst has suc...\n",
       "Name: no_punc, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tok = RegexpTokenizer(r'\\w+' )\n",
    "\n",
    "pos_tweets['no_punc'] = pos_tweets['lower'].apply(lambda x: ' '.join(reg_tok.tokenize(x)))\n",
    "pos_tweets.no_punc.head()\n",
    "\n",
    "\n",
    "# How not to remove the hash and the tags in this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above output, all the punctuation, including ‘#’ and ‘@’, has been removed from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Removal of Stop Words\n",
    "\n",
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    followfriday france_inte pkuchly57 milipol_par...\n",
       "1    lamb2ja hey james odd please call contact cent...\n",
       "2    despiteofficial listen last night bleed amazin...\n",
       "3                                     97sides congrats\n",
       "4    yeaaaah yippppy accnt verified rqst succeed go...\n",
       "Name: no_stop, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets['no_stop'] = pos_tweets['no_punc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "pos_tweets.no_stop.head()\n",
    "\n",
    "#Pipeline \n",
    "# 1 -> lower case()\n",
    "# 2 -> Punctuation()\n",
    "# 3 -> Stop word()\n",
    "# 4(optional) -> Common words()\n",
    "# 5(optional) -> Rare word\n",
    "# 6 -> Tokenizer (very important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4 Common word removal\n",
    "\n",
    "Previously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "co        1195\n",
       "http       845\n",
       "https      331\n",
       "follow     287\n",
       "thanks     261\n",
       "love       225\n",
       "u          215\n",
       "good       189\n",
       "like       186\n",
       "amp        174\n",
       "dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.Series(''.join(pos_tweets['no_stop']).split()).value_counts()[:10]\n",
    "freq\n",
    "\n",
    "## Test the join part ( series does not have the split() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s remove these words as their presence will not of any use in classification of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    followfriday france_inte pkuchly57 milipol_par...\n",
       "1    lamb2ja hey james odd please call contact cent...\n",
       "2    despiteofficial listen last night bleed amazin...\n",
       "3                                     97sides congrats\n",
       "4    yeaaaah yippppy accnt verified rqst succeed go...\n",
       "Name: no_common, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = list(freq.index)\n",
    "pos_tweets['no_common'] = pos_tweets['no_stop'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "pos_tweets['no_common'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Rare words removal\n",
    "\n",
    "Similarly, just as we removed the most common words, this time let’s remove rarely occurring words from the text. Because they’re so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bing             1\n",
       "nikeindonesia    1\n",
       "uhuh             1\n",
       "batman           1\n",
       "anz              1\n",
       "emilysmith_91    1\n",
       "thispiggy        1\n",
       "allows           1\n",
       "8fiypq6kkn       1\n",
       "manipalhealth    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rare = pd.Series(' '.join(pos_tweets['no_common']).split()).value_counts()[-10:]\n",
    "rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    followfriday france_inte pkuchly57 milipol_par...\n",
       "1    lamb2ja hey james odd please call contact cent...\n",
       "2    despiteofficial listen last night bleed amazin...\n",
       "3                                     97sides congrats\n",
       "4    yeaaaah yippppy accnt verified rqst succeed go...\n",
       "Name: no_common, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rare = list(rare.index)\n",
    "pos_tweets['no_rare'] = pos_tweets['no_common'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "pos_tweets['no_common'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Spelling correction\n",
    "\n",
    "We’ve all seen tweets with a plethora of spelling mistakes. Our timelines are often filled with hastly sent tweets that are barely legible at times.\n",
    "\n",
    "In that regard, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n",
    "\n",
    "To achieve this we will use the textblob library. If you are not familiar with it, you can check my previous article on ‘NLP for beginners using textblob’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    followfriday france_inte pkuchly57 milipol_par...\n",
       "1    lamb2ja hey james odd please call contact cent...\n",
       "2    despiteofficial listen last night bleed amazin...\n",
       "3                                       sides congress\n",
       "4    yeaaaah yippppy accent verified rest succeed g...\n",
       "Name: no_common, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "pos_tweets['no_common'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the words are corrected accurately ('accnt -> accent'); also some are not ('rqst' -> 'rest')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Tokenization - Again\n",
    "\n",
    "Tokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Lamb2ja',\n",
       " 'Hey',\n",
       " 'James',\n",
       " '!',\n",
       " 'How',\n",
       " 'odd',\n",
       " ':/',\n",
       " 'Please',\n",
       " 'call',\n",
       " 'our',\n",
       " 'Contact',\n",
       " 'Centre',\n",
       " 'on',\n",
       " '02392441234',\n",
       " 'and',\n",
       " 'we',\n",
       " 'will',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'assist',\n",
       " 'you',\n",
       " ':)',\n",
       " 'Many',\n",
       " 'thanks',\n",
       " '!']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "twtkr = TweetTokenizer()\n",
    "twtkr.tokenize(pos_tweets['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Select the last 10 tweets in the dataset (pos_tweets), then use following tokenizers to tokenize each tweet and output the results.\n",
    "- Regxp Tokenizer (reg_tok)\n",
    "- Tweet Tokenizer (twtkr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4990    [Niallll_1Dx, it, s, a, cool, video, i, love, ...\n",
       "4991    [Season, 11, set, Have, a, nice, Friday, SPNFa...\n",
       "4992                    [theguyliner, Trishie_D, cool, D]\n",
       "4993                                 [fczbkk, Exactly, D]\n",
       "4994    [kevinngmingyuan, peasant, seats, to, watch, a...\n",
       "4995    [chriswiggin3, Chris, that, s, great, to, hear...\n",
       "4996    [RachelLiskeard, Thanks, for, the, shout, out,...\n",
       "4997                 [side556, Hey, Long, time, no, talk]\n",
       "4998    [staybubbly69, as, Matt, would, say, WELCOME, ...\n",
       "4999    [DanielOConnel18, you, could, say, he, will, h...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We already defined both tokenizer, now we just need to call them\n",
    "\n",
    "\n",
    "pos_tweets['text'][-10:].apply(lambda x: reg_tok.tokenize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4990    [@Niallll_1Dx, it's, a, cool, video, i, love, ...\n",
       "4991    [Season, 11, set, ♡, Have, a, nice, Friday, #S...\n",
       "4992              [@theguyliner, @Trishie_D, cool, ., :D]\n",
       "4993                            [@fczbkk, Exactly, !, :D]\n",
       "4994    [@kevinngmingyuan, peasant, seats, to, watch, ...\n",
       "4995    [@chriswiggin3, Chris, ,, that's, great, to, h...\n",
       "4996    [@RachelLiskeard, Thanks, for, the, shout-out,...\n",
       "4997    [@side556, Hey, !, :), Long, time, no, talk, ...]\n",
       "4998    [@staybubbly69, as, Matt, would, say, ., WELCO...\n",
       "4999    [@DanielOConnel18, you, could, say, he, will, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tweets['text'][-10:].apply(lambda x: twtkr.tokenize(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Stemming\n",
    "\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #followfriday @france_int @pkuchly57 @milipol_...\n",
       "1    @lamb2ja hey james! how odd :/ pleas call our ...\n",
       "2    @despiteoffici we had a listen last night :) a...\n",
       "3                                   @97side congrat :)\n",
       "4    yeaaaah yippppy!!! my accnt verifi rqst has su...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "st = SnowballStemmer(\"english\")\n",
    "pos_tweets['text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data is not very good from stemming because of the spelling and non plain English tokens (handles, tags, ...). However, to further prove that the stemming is working, try code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(st.stem('running'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the native Snowball stemmer; we can also use other stemmer within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Snowball stemmer on generously is: generous.\n",
      "Use Porter stemmer on generously is: gener.\n"
     ]
    }
   ],
   "source": [
    "print('Use Snowball stemmer on generously is: %s.' % str(SnowballStemmer(\"english\").stem(\"generously\")))\n",
    "print('Use Porter stemmer on generously is: %s.' % str(SnowballStemmer(\"porter\").stem(\"generously\")))\n",
    "\n",
    "# Stemming : porter is better than the snowball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Lemmatization\n",
    "\n",
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. **Therefore, we usually prefer using lemmatization over stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    #FollowFriday @France_Inte @PKuchly57 @Milipol...\n",
       "1    @Lamb2ja Hey James! How odd :/ Please call our...\n",
       "2    @DespiteOfficial we had a listen last night :)...\n",
       "3                                 @97sides CONGRATS :)\n",
       "4    yeaaaah yippppy!!! my accnt verified rqst ha s...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization is similar to stemming \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "pos_tweets['text'][:5].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, tweets may not be the best for demonstrating *lemmatization*, so we check following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WordNet',\n",
       " 'superficially',\n",
       " 'resembles',\n",
       " 'a',\n",
       " 'thesaurus',\n",
       " 'in',\n",
       " 'that',\n",
       " 'it',\n",
       " 'group',\n",
       " 'word',\n",
       " 'together',\n",
       " 'based',\n",
       " 'on',\n",
       " 'their',\n",
       " 'meaning',\n",
       " 'However',\n",
       " 'there',\n",
       " 'are',\n",
       " 'some',\n",
       " 'important',\n",
       " 'distinction']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sent = 'WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. \\\n",
    "However, there are some important distinctions .'\n",
    "[wordnet_lemmatizer.lemmatize(w) for w in reg_tok.tokenize(my_sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define the Part-of-Speech in lemmatization. See code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.10 Part-of-Speech Tagging\n",
    "\n",
    "Part-of-speech tagging is one of the most important text analysis tasks used to classify words into their part-of-speech and label them according the tagset which is a collection of tags used for the pos tagging. Part-of-speech tagging also known as word classes or lexical categories. Detailed definition can be found on [Wikipedia](https://en.wikipedia.org/wiki/Part-of-speech_tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Due', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('nature', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('tagger', 'NN'),\n",
       " ('it', 'PRP'),\n",
       " ('works', 'VBZ'),\n",
       " ('best', 'RB'),\n",
       " ('when', 'WRB'),\n",
       " ('trained', 'VBN'),\n",
       " ('over', 'IN'),\n",
       " ('sentence', 'NN'),\n",
       " ('delimited', 'VBN'),\n",
       " ('input', 'NN')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Due to the nature of this tagger, it works best when trained over sentence delimited input.'\n",
    "from nltk import pos_tag\n",
    "pos_tag([w for w in reg_tok.tokenize(s)])\n",
    "\n",
    "# Returing parts of speech \n",
    "#where JJ is adjective  , DT - Deliminators , NN - nouns etc. Google for the part of speech tagging tree. \n",
    "\n",
    "# Recognizing the parts of speech could be at any stage. \n",
    "\n",
    "# Please go through Spacy and Gensin for textual data (libraries) , pattern(python2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pos_tag by default takes **a list of strings** as input; if you input a string, pos_tag will split it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "We are going to load the book *Monty Python and Holy Grail* from the embedded text in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Your code here\n",
    "# difference between sorted(set(w.lower()for w in text1))\n",
    "# sorted(w.lower() for w in set(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 't',\n",
       " 'x',\n",
       " 'y']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' book Monty Python and Holy Grail from the embedded text in NLTK'\n",
    "\n",
    "R1 = sorted(set(w.lower()for w in text))\n",
    "#print(len(R1))\n",
    "R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'a',\n",
       " 'b',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'h',\n",
       " 'i',\n",
       " 'k',\n",
       " 'k',\n",
       " 'l',\n",
       " 'l',\n",
       " 'm',\n",
       " 'm',\n",
       " 'n',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 't',\n",
       " 't',\n",
       " 'x',\n",
       " 'y']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2 = sorted(w.lower() for w in set(text))\n",
    "print(len(R2))\n",
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.\n",
    "Define sent to be the list of words \n",
    "\n",
    "    ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']. \n",
    "\n",
    "Now write code to perform the following tasks:\n",
    "- Print all words beginning with **se**\n",
    "- Print all words longer than **four** characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-166-88fd1cf4f1fc>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-166-88fd1cf4f1fc>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    if x[i][:2] == 's'\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning a variable for the list of words.\n",
    "x = ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']\n",
    "# Generating a list for all the words starting with the letters 'se'\n",
    "t = [word for word in x if word.startswith('se')]\n",
    "print(t)\n",
    "# Generating a list of only those whords whose len is > 4\n",
    "y = [word for word in x if len(word)>4]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Tutorial\n",
    "\n",
    "In this tutorial, we complete **basic feature extraction** and **basic text pre-processing** in this tutorial. We will continue with **advanced text processing** in Part 2 of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
